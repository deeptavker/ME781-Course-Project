from keras.preprocessing.text import Tokenizer
import numpy as np
import re
from keras.utils import to_categorical
import io
import json
import random
from pickle import load
from keras.models import load_model
from keras.utils import Sequence



class Data_Generator(Sequence) :
    def __init__(self, fname, m) :
        self.indices = np.arange(0,m)
        self.fname = fname
        # actual name will be fname + _ + i/o + _ + idx + .npy ( idx -> 0 to m-1 )
        self.m = m
        # m is the number of files generated by pre-batching of npy files

    def __len__(self) :
        return m


    def __getitem__(self, idx) :

        temp_id = self.indices[idx]

        batch_x = np.load(self.fname + '_i_' + str(temp_id) + '.npy')
        batch_y = np.load(self.fname + '_o_' + str(temp_id) + '.npy')

        return (batch_x, batch_y)

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        np.random.shuffle(self.indices)

m = 32
datagenerator = Data_Generator('../io_files/io_128', m)

model = load_model('../var_files/word_pred_Model4.hdf5')

model.fit_generator(datagenerator, epochs = 600)

model.save('../var_files/word_pred_Model4.hdf5')

# do we need to mention steps_per_epoch or does it take it from __len__
# shuffle the indices for batch parsing? on_epoch_end()




